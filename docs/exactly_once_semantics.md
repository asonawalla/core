Design Brief: Exactly-once Semantics in Gazette
===============================================

Overview
--------

Gazette defaults to exactly-once processing semantics over messages, sometimes also known
as "effectively-once". The formal guarantees that Gazette offers are that:

* A message `A` will be processed in exactly one completed consumer transaction.
* Any updates to the consumer's stateful store which are derived from `A` will commit exactly once.
* Any published messages `B`, derived from `A`, will be read exactly once by any downstream
  read-committed reader (including other consumers).

These guarantees also cascade through multiple consumers, providing an overall end-to-end
assurance that the effects of a message will be applied just once regardless of how that
message is transformed, decomposed, aggregated, re-combined, etc as it passes from consumer
to consumer.

Message UUIDs
-------------

Journal appends in Gazette are at-least-once, meaning a particular message byte serialization
may commit to a journal multiple times.

Messages in Gazette are sequenced using [RFC 4122 v1 UUIDs](https://tools.ietf.org/html/rfc4122),
which are composed of:
 * A "node ID" which identifies the unique message source
   (also known within Gazette as a [ProducerID](../message/interfaces.go#L33)).
 * A timestamp, with resolution to 100 nanoseconds.
 * A clock sequence, which provides further bits to distinguish UUIDs having the same
   node ID and timestamp. Gazette re-purposes some of these bits to use as
   [flags](../message/uuid.go#L125).

Within Gazette, UUID timestamps and clock sequence bits are generated from a
strictly monotonic [Clock](../message/uuid.go#L59) which ticks with each UUID generated.
Every [Publisher](../message/publisher.go#L13) instance likewise draws a new and
random `ProducerID`.

The combination of these properties allows a reader to efficiently de-duplicate
messages by tracking the largest `Clock` seen for each `ProducerID`. Read messages
having smaller clocks are presumed duplicates and can be discarded.

UUIDs are also used to represent transaction semantics via encoded flags. An
application can author a set of messages which will be atomically applied
or rolled-back, by publishing each "pending" message with flag `CONTINUE_TXN`,
and applying them via an `ACK_TXN` message, which commits all pending
messages having a smaller `Clock` (and rolls-back those having a larger one).

[ReadUncommittedIter](../message/reader.go#L21) reads direct uncommitted messages from a
journal, and may include duplicates and pending messages which have not yet committed,
or may be rolled back. 

[Sequencer](../message/sequencer.go) observes read-uncommitted messages from journals,
and sequences them into read-committed messages. For efficiency it uses a tunable
ring-buffer of read messages so that, in most cases, messages can be directly
returned from the ring upon reading an `ACK_TXN` which commits them. In cases where
the ring buffer is insufficient, `Sequencer` will re-read the relevant portion of the
journal to deliver acknowledged messages. An advantage of the design is that no
head-of-line blocking occurs: committed messages are immediately dequed upon reading
their corresponding `ACK_TXN`, even if they're interleaved with still-pending messages
of other producers. Gazette is also able to guarantee that downstream consumers of a
published-to journal will process the entire set of acknowledged messages within a
single consumer transaction (this guarantee does not extend beyond a single journal,
however).

[ReadCommittedIter](../message/reader.go#L131) composes a `ReadUncommittedIter` with a
`Sequencer` to read committed messages of a journal.

Gazette messages are arbitrary user-defined types, and journals themselves
hold only raw user data. Gazette therefore asks that user [Message](../message/interfaces.go#L15)
types help with representation by taking, serializing, and when asked, returning
UUIDs generated by Gazette. UUIDs may also be directly useful to users, as they're
universally unique and they encode a precise publishing timestamp. In some cases,
user types may be unable to represent a UUID. In these cases, the `Message`
interface can be implemented as no-ops to opt the type out of exactly-once
processing, falling back to at-least-once semantics.

Consumer Shard Transaction Lifecycle
------------------------------------

Each shard of a consumer [Application](../consumer/interfaces.go#L99) processes
messages in pipelined transactions. While each transaction runs:

 * At least one Message will be processed from a source journal.
 * Reads and modifications of a [Store](../consumer/interfaces.go#L52) are made,
   scoped to a transaction provided by the store (eg, a RocksDB WriteBatch, or a SQL transaction).
 * A number of uncommitted messages may be published to downstream journals,
   identified by a `Publisher` & `ProducerID` which is unique to the shard assignment.

So far, no effects of the transaction are yet observable by a read-committed
reader or from the `Store` itself. Eventually, either because further messages are
not immediately available for processing or because of a transaction timeout,
the consumer transaction will begin to commit.

At this point a consumer [Checkpoint](../consumer/protocol/protocol.proto#L214) is assembled,
which consists of:

 * Offsets of each source journal through which uncommitted messages have been read.
 * `ProducerStates` of each producer tracked by the `Sequencer`.
 * "Intents" of messages to be written, which acknowledge uncommitted messages
   published during the transaction (aka "ACK intents").

The checkpoint is added to and then commits with the store transaction. Once the
transaction commits (and never before), ACK intents of the checkpoint are written
to inform downstream readers that pending messages have committed.

On process assignment / shard startup, shards recover the most recent `Checkpoint`
from their `Store` and immediately re-publish its ACK intents. If a fault previously
occurred after the transaction committed but before all intents were published, then
this ensures delivery of those acknowledgements. If a fault instead occurred in the
middle of a transaction, this rolls-back any pending messages of that abandoned
transaction.

Every `Publisher` instance uses a unique `ProducerID`, which must be independently
tracked by all `Sequencers`. While the state for each producer is lightweight (just
a byte offset and `Clock`), this can add up over time. Consumers employ pruning to
age-out states of `ProducerID`s which are no longer active.

Store Fencing
-------------

Shards are assigned (and re-assigned) across many distributed consumer processes
which may come, go, and fail arbitrarily. There may be races where a shard which
is re-assigned from process `Old` to `New` may be processed by both simultaneously
(here, `Old` is sometimes referred to as a "zombie"). During this race there's a
potential for:
 * `Old` to commit transaction `T1`, then
 * `New` to recover `T1`, then
 * `Old` to commit `T2`, write ACK intents, and exit.

This breaks exactly-once semantics: some derived messages may be written more than
once, and messages may be applied to the store multiple times. To account for this,
it must be the case that after `New` recovers `T1`, it's no longer possible for
`Old` to commit a transaction. In other words, `New` must place a
[write fence](https://en.wikipedia.org/wiki/Memory_barrier) on startup.

External `Stores` may leverage features of a remote store to implement fences.
For example, [StoreSQL](../consumer/store_sql.go) increments a SQL `fence` on
`RestoreCheckpoint`, causing a future commit of a zombie process to fail.

Embedded `Stores` such as [StoreJSONFile](.../consumer/store_json_file.go) or
[StoreRocksDB](../consumer/store-rocksdb/store_rocksdb.go), which make use of
[recovery logs](https://godoc.org/github.com/gazette/core/consumer/recoverylog)
use journal "registers" to implement fencing across processes.

Recovery Logs and Register Fencing 
----------------------------------

Journal [Registers](../broker/protocol/protocol.proto#L351) are (very small) collections
of arbitrary keys and values which are associated to a journal, and participate in that
journal's transactional append machinery. Append RPCs may check that registers match an
expectation in order to proceed, and so long as they append at least one byte, they may
update the registers of the journal.

When a shard process assignment becomes primary, it completes recovery log playback by
[injecting a "handoff"](../consumer/recoverylog/playback.go#L334), which takes over
sequencing of recorded operations in the log and places an updated "author" register
fence upon it. As it processes transactions, its [Recorder](../consumer/recoverylog/recorder.go#L23)
to the log verifies the "author" fence it previously placed with each recorded operation.
When the *next* assigned process finishes playback and injects a hand-off, it fences off
any further appends of this `Recorder` from applying to the journal, ensuring it can no
longer commit a consumer transaction checkpoint.
